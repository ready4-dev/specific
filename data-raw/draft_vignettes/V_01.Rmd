---
title: "Exploratory Analysis Workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Exploratory Analysis Workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Note: **This vignette uses fake data** - it is for illustrative purposes only and should not be used to inform decision making.

The steps in this exploratory analysis workflow may need to be performed iteratively, both in order to identify the optimal model types, predictors and covariates to use and modify default values to ensure model convergence. 

```{r message=FALSE, warning=FALSE}
library(specific)
```

## Import data

We start by ingesting our data in the form of a `ScorzProfile` object. As this example uses EQ-5D the type of `ScorzProfile` object we import is a `ScorzEuroQol5`. The following step is explained in more detail in [this vignette from the scorz pacakge](https://ready4-dev.github.io/scorz/articles/V_02.html).

```{r}
x <- ready4use::Ready4useRepos(dv_nm_1L_chr = "fakes", # Get direct from scorz release repo
                               dv_ds_nm_1L_chr = "https://doi.org/10.7910/DVN/W95KED",
                               dv_server_1L_chr = "dataverse.harvard.edu") %>%
  ingest(fls_to_ingest_chr = "ymh_eq5d_dyad_r4",
         metadata_1L_lgl = F) %>%
  youthvars::YouthvarsSeries(a_Ready4useDyad = .,
                                id_var_nm_1L_chr = "uid",
                                timepoint_var_nm_1L_chr = "Timepoint",
                                timepoint_vals_chr = unique(.@ds_tb$Timepoint)) %>%
  scorz::ScorzEuroQol5(a_YouthvarsProfile = .,
                   country_1L_chr = "UK",
                   instrument_version_1L_chr = "5L",
                   itm_var_nms_chr = c("eq5dq_MO", "eq5dq_SC", "eq5dq_UA", "eq5dq_PD", "eq5dq_AD")) %>%
  renew()
```

## Inspect data
```{r echo = F, message=FALSE}
ds_tb <- x %>% 
  procureSlot("a_YouthvarsProfile") %>%
  procureSlot("a_Ready4useDyad") %>%
  procureSlot("ds_tb")
```

The dataset we are using has a total of `r nrow(ds_tb)` records at two timepoints on `r length(ds_tb$uid %>% unique())` study participants. The first six records are reproduced below.

```{r inputds, echo=F, eval = knitr::is_html_output(), results='asis'}
x %>% 
  procureSlot("a_YouthvarsProfile") %>%
  procureSlot("a_Ready4useDyad") %>%
  exhibit(display_1L_chr = "head")
```

To see how the data in the `a_YouthvarsProfile` slot of `x` (a `YouthvarsSeries` object) can be described in more detail [read this vignette from the youthvars package](https://ready4-dev.github.io/youthvars/articles/V_02.html).

## Create SpecificProject

To start with we create a new instance of a `SpecificProject` object that contains the data we have just ingested.

```{r}
x <- SpecificProject(a_ScorzProfile = x) # REVERT TO YOUTHVARS
# Add descriptives methods
```


## Specify parameters

In preparation for exploring our dataset, we need to declare a set of model parameters in a `SpecificParameters` object. This can be done in one step, or in sequential steps. In this example, we will proceed sequentially.

### Dataset and dependent variable
The first parameter that we declare is the dependent variable (total EQ-5D utility score) for the models we will be exploring.

```{r}
y <- SpecificParameters(#a_ScorzProfile = x,
                        depnt_var_nm_1L_chr = "eq5d_total_w")
```

### Candidate predictors

We add the names of candidate predictor variables to our `SpecificParameters` object.

```{r}
y <- renewSlot(y,
               slot_nm_1L_chr = "candidate_predrs_chr",
               new_val_xx = c("K10_int","Psych_well_int")) 

```

We next add meta-data about each candidate predictor variable in the form of a `specific_predictors` object.

```{r}
y <- renewSlot(y, 
               slot_nm_1L_chr = "predictors_lup", 
               new_val_xx = make_pt_specific_predictors(short_name_chr = c("K10_int","Psych_well_int"),
                                              long_name_chr = c("Kessler Psychological Distress - 10 Item Total Score",
                                                   "Overall Wellbeing Measure (Winefield et al. 2012)"),
                                              min_val_dbl = c(10,18),
                                              max_val_dbl = c(50,90),
                                              class_chr = "integer",
                                              increment_dbl = 1,
                                              class_fn_chr = "as.integer",
                                              mdl_scaling_dbl = 0.01,
                                              covariate_lgl = F) %>%
                 specific_predictors())
```

The `specific_predictors` object that we have added to `y` can be retrieved using the `procureSlot` method.

```{r}
procureSlot(y,
            "predictors_lup")
```

### Covariates

We also specify the covariates that we aim to explore in conjunction with each candidate predictor.

```{r}
y <- renewSlot(y,
               slot_nm_1L_chr = "candidate_covars_chr",
               new_val_xx = c("d_sex_birth_s", "d_age",  "d_sexual_ori_s", "d_relation_s", "d_studying_working"))
```

### Candidate models

We now add details about the candidate models we will explore in the form of a `specific_models` object.

```{r}
y <- renewSlot(y,
               slot_nm_1L_chr = "candidate_mdls_lup",
               new_val_xx = get_cndts_for_mxd_mdls() %>%
                 specific_models())
```

The `specific_models` object that we have just added is reproduced below.

```{r}
procureSlot(y,
            "candidate_mdls_lup") %>%
    ready4show::print_table(output_type_1L_chr = "HTML",
                            caption_1L_chr = "Model types lookup table")
```

By default, all the models in the `specific_models` object will be explored. However, if we just wish to explore a subset of these we can specify the names (corresponding to those in the `short_name_chr` column) of the models we wish to test, adding it to the `candidate_mdls_chr` slot of `y`.

```{r}
x <- renewSlot(y,
               slot_nm_1L_chr = "candidate_mdls_chr",
               new_val_xx = c("OLS_NTF","OLS_LOG","OLS_LOGIT"))
```

We must now also specify the relevant prefix (the first three letters of the names in the `short_name_chr` column of the `specific_models` object) using the `candidate_mdl_pfcs_chr` slot of `y`.

```{r}
y <- renewSlot(y,
               slot_nm_1L_chr = "candidate_mdl_pfcs_chr",
               new_val_xx = c("OLS"))
```

### Other parameters

Depending on the type of analysis we plan on undertaking, we can also specify parameters such as the number of folds to use in cross validation and the maximum number of model runs to allow.

```{r}
y <- renewSlot(y,
               slot_nm_1L_chr = "folds_1L_int",
               new_val_xx = 10L) %>%
  renewSlot(slot_nm_1L_chr = "max_mdl_runs_1L_int",
               new_val_xx = 300L)
```

We can also specify a seed to ensure reproducibility of results.

```{r}
y <- renewSlot(y,
               slot_nm_1L_chr = "seed_1L_int",
               new_val_xx = 1234L)
```

Finally, we add `y` to `x`,

```{r}
x <- renewSlot(x,
               slot_nm_1L_chr = "b_SpecificParameters",
               new_val_xx = y)
```

## Model testing


### Set-up workspace 

We create a directory in which we will write all output.

```{r warning = F}
path_to_write_to_1L_chr <- "Output" 
dir.create(path_to_write_to_1L_chr)
```

```{r warning=F, message=F}
if(!is.na(x@candidate_mdls_chr[1])){
  mdl_types_chr <- x@candidate_mdls_chr
}else{
  mdl_types_chr <- NULL
}
if(!is.na(x@candidate_mdl_pfcs_chr[1])){
  choose_from_pfx_chr <- x@candidate_mdl_pfcs_chr
}else{
  choose_from_pfx_chr <- NULL
}
ds_smry_ls <- make_ds_smry_ls(candidate_predrs_chr = x@candidate_predrs_chr,
                              candidate_covar_nms_chr = x@candidate_covars_chr,
                              depnt_var_nm_1L_chr = x@depnt_var_nm_1L_chr,
                              dictionary_tb = x@a_ScorzProfile@a_YouthvarsProfile@a_Ready4useDyad@dictionary_r3, #Dyad
                              id_var_nm_1L_chr = x@a_ScorzProfile@a_YouthvarsProfile@id_var_nm_1L_chr, # Profile
                              round_var_nm_1L_chr = x@a_ScorzProfile@a_YouthvarsProfile@timepoint_var_nm_1L_chr, # Series
                              round_bl_val_1L_chr = x@a_ScorzProfile@a_YouthvarsProfile@timepoint_vals_chr[1],
                              predictors_lup = x@predictors_lup)
mdl_smry_ls <- make_mdl_smry_ls(mdl_types_lup = x@candidate_mdls_lup,
                                mdl_types_chr = mdl_types_chr,
                                choose_from_pfx_chr = choose_from_pfx_chr,
                                folds_1L_int = x@folds_1L_int,
                                max_nbr_of_boruta_mdl_runs_int = x@max_mdl_runs_1L_int)
cmprsns_ls <- write_mdl_cmprsn(scored_data_tb = x@a_ScorzProfile@a_YouthvarsProfile@a_Ready4useDyad@ds_tb,
                                ds_smry_ls = ds_smry_ls,
                                mdl_smry_ls = mdl_smry_ls,
                                output_data_dir_1L_chr = path_to_write_to_1L_chr,
                                seed_1L_int = x@seed_1L_int)
```

The `write_mdl_cmprsn` function will write each model to be tested to a new sub-directory of "`r path_to_write_to_1L_chr`" called "A_Candidate_Mdls_Cmprsn". Also written to that sub-directory are a number of plots for each model. The `write_mdl_cmprsn` function also outputs a table summarising the performance of each of the candidate models.

```{r mdl_cmprsn, eval = knitr::is_html_output(), results='asis'}
cmprsns_ls$mdl_smry_ls$smry_of_sngl_predr_mdls_tb %>%
  ready4show::print_table(output_type_1L_chr = "HTML",
                          caption_1L_chr = "Comparison of model types for highest correlated predictor using baseline data",
                          mkdn_tbl_ref_1L_chr = "tab:mdl_cmprsn")
```

We can now identify the highest performing model in each category of candidate model based on the testing R^2^ statistic (RsquaredP in the previous table).

```{r}
cmprsns_ls$mdl_smry_ls$prefd_mdl_types_chr
```
We can override these automated selections and instead incorporate other considerations (possibly based on judgments informed by visual inspection of the plots and the desirability of constraining predictions to a maximum value of one). We do this in the following command, specifying new preferred model types, in descending order of preference.

```{r}
cmprsns_ls$mdl_smry_ls$prefd_mdl_types_chr <- c("OLS_NTF", "BET_CLL")
```

### Use most preferred model to compare all candidate predictors
We can now compare all of our candidate predictors (with and without candidate covariates) using the most preferred model type.

```{r message=FALSE, results='hide', warning=FALSE}
cmprsns_ls <- write_predr_and_covars_cmprsn(scored_data_tb = x@a_ScorzProfile@a_YouthvarsProfile@a_Ready4useDyad@ds_tb,
                                            bl_tb = cmprsns_ls$bl_tb, # Create a ds_copies slot
                                            ds_smry_ls = cmprsns_ls$ds_smry_ls,
                                            mdl_smry_ls = cmprsns_ls$mdl_smry_ls,
                                            output_data_dir_1L_chr = path_to_write_to_1L_chr,
                                            seed_1L_int = x@seed_1L_int)
```

Now, we compare the performance of single predictor models of our preferred model type (in our case, a `r cmprsns_ls$mdl_smry_ls$mdl_types_lup %>% ready4::get_from_lup_obj(target_var_nm_1L_chr = "long_name_chr",match_value_xx = cmprsns_ls$mdl_smry_ls$prefd_mdl_types_chr[1], match_var_nm_1L_chr = "short_name_chr", evaluate_1L_lgl = F)`) for each candidate predictor. The call to the `write_predr_and_covars_cmprsn`{.R} function saved the tested models along with model plots in the "B_Candidate_Predrs_Cmprsn" sub-directory of "Output". These results are also viewable as a table.

```{r}
cmprsns_ls$mdl_smry_ls$predr_cmprsn_tb %>%
    ready4show::print_table(output_type_1L_chr = "HTML",
                          caption_1L_chr = "Comparison of all candidate predictors using preferred model")
```
The call to the `write_predr_and_covars_cmprsn`{.R} function also saved single predictor R model objects (one for each candidate predictors) along with the two plots for each model in the "C_Predrs_Sngl_Mdl_Cmprsn" sub-directory of "Output". The performance of each single predictor model can also be summarised in a table.

```{r}
cmprsns_ls$mdl_smry_ls$smry_of_mdl_sngl_predrs_tb %>%
    ready4show::print_table(output_type_1L_chr = "HTML",
                          caption_1L_chr = "Preferred single predictor model performance by candidate predictor")
```
Updated versions of each of the models in the previous step (this time with covariates added) are saved to the "D_Predr_Covars_Cmprsn" subdirectory of "Output" and we can summarise the performance of each of the updated models, along with all signficant model terms, in a table.

```{r}
cmprsns_ls$mdl_smry_ls$mdls_with_covars_smry_tb %>%
    ready4show::print_table(output_type_1L_chr = "HTML",
                            caption_1L_chr = "Performance of preferred models with covariate predictors by candidate predictor", 
                          use_lbls_as_col_nms_1L_lgl = T)
```
We can now identify which, if any, of the candidate covariates we previously specified are significant predictors in any of the models.

```{r}
cmprsns_ls$mdl_smry_ls$signt_covars_chr
```
We can override the covariates to select, potentially because we want to select only covariates that are significant for all or most of the models. However, in the below example we have opted not to do so and continue to use `r ifelse(is.na(mdl_smry_ls$signt_covars_chr),"no covariates",paste0(mdl_smry_ls$signt_covars_chr, collapse = "and "))` as selected by the algorithm in the previous step.

```{r}
# cmprsns_ls$mdl_smry_ls$prefd_covars_chr <- c("COVARIATE WE WANT TO USE", "ANOTHER COVARIATE")
```

### Test preferred model with preferred covariates for each candidate predictor
We now conclude our model testing by rerunning the previous step, except confining our covariates to those we prefer.

```{r message=FALSE, results='hide', warning=FALSE, fig.show ='hide'}
outp_smry_ls <- write_mdls_with_covars_cmprsn(scored_data_tb = x@a_ScorzProfile@a_YouthvarsProfile@a_Ready4useDyad@ds_tb,
                                           bl_tb = cmprsns_ls$bl_tb,
                                           ds_smry_ls = cmprsns_ls$ds_smry_ls,
                                           mdl_smry_ls = cmprsns_ls$mdl_smry_ls,
                                           output_data_dir_1L_chr = path_to_write_to_1L_chr,
                                           seed_1L_int = x@seed_1L_int)
```

The previous call to the `write_mdls_with_covars_cmprsn`{.R} function saves the tested models along with the two plots for each model in the "E_Predrs_W_Covars_Sngl_Mdl_Cmprsn" sub-directory of "Output". 

## Apply preferred model types and predictors to longitudinal data
The next main step is to use the preferred model types and covariates identified from the preceding analysis of cross-sectional data in longitudinal analysis. 

### Longitudinal mixed modelling
Next we create longitudinal and mixed versions of each of the models we specified in the previous step. Note, that in many cases both the `prior_ls` and `control_ls` arguments can be set to `NULL` (which may speed up execution). However, in this example doing so would result in warning messages suggesting a change to the adapt_delta control value (default = 0.8). We have therefore passed a value to the `control_ls` argument that addresses this issue.

```{r eval = F}
outp_smry_ls <- write_ts_mdls_from_alg_outp(outp_smry_ls,
                                            predictors_lup = predictors_lup ,
                                            utl_min_val_1L_dbl = 0.03,
                                            backend_1L_chr = "cmdstanr",
                                            new_dir_nm_1L_chr = "F_TS_Mdls",
                                            iters_1L_int = 4000L,
                                            prior_ls = NULL, 
                                            control_ls = list(adapt_delta = 0.99))
```

The `write_ts_mdls_from_alg_outp`{.R} function writes the models it tests to the "F_TS_Mdls" sub-directory of "Output" along with three plots for each model. 

## Purge dataset copies
Because the files created in analysis are large, multiple objects containing copies of the source dataset have been saved to our output directory during the analysis process. We therefore need to delete all of these copies.

```{r eval=F}
write_to_delete_mdl_fls(outp_smry_ls)
outp_smry_ls$scored_data_tb <- NULL
```

## Save work
Finally, we can save a complete record of our analysis.

```{r eval=F}
saveRDS(outp_smry_ls,paste0(outp_smry_ls$path_to_write_to_1L_chr,"/I_ALL_OUTPUT_.RDS"))
```
